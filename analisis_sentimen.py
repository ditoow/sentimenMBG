# -*- coding: utf-8 -*-
"""Analisis Sentimen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sthTfjjgxsaQZkpy7ZQAGmWp1ToRs7nn
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import re
import nltk
from nltk.corpus import stopwords
from sklearn.preprocessing import LabelEncoder
import sklearn
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
import pickle


df = pd.read_csv('data/dataset.csv')

#cek untuk tau data awalnya ini kayak tipe data dan ada yg missing values nggak
df.info()

#buat lihat mean, median dan utk menggali informasi dari data
df.describe()

#pemeriksaan missing values pakainya persen
missing_percent = df.isnull().mean() * 100
missing_percent[missing_percent > 0]

#ada missing valuesnya tapi cuman dikit jd gaperlu dihapus
# Handle missing values (text-based dataset)
df['author'] = df['author'].fillna('unknown_author')
df['videoID'] = df['videoID'].fillna('unknown_video')

# teks cleaning
print("\n" + "="*60)
print("MEMULAI TEKS CLEANING UNTUK KOLOM 'comment'")
print("="*60)

# Download stopwords jika belum ada
try:
    stop_words_id = set(stopwords.words('indonesian'))
except:
    nltk.download('stopwords')
    stop_words_id = set(stopwords.words('indonesian'))

def clean_youtube_comment(text):
    if not isinstance(text, str):
        return ""

    # 1. Ubah ke huruf kecil
    text = text.lower()

    # 2. Hapus URL, mention, dan hashtag
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#(\w+)', r'\1', text)

    # 3. Hapus karakter YouTube khusus
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\(.*?\)', '', text)

    # 4. Hapus karakter berulang (>2)
    text = re.sub(r'(.)\1{2,}', r'\1', text)

    # 5. Hapus angka dan tanda baca
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)

    # 6. Normalisasi spasi
    text = re.sub(r'\s+', ' ', text).strip()

    # 7. Hapus stopwords tapi pertahankan kata negasi penting
    important_words = {'tidak', 'bukan', 'jangan', 'belum', 'tak', 'ga', 'gak', 'nggak'}
    sentiment_whitelist = {'keren','mantap','layak','berhasil', 'bagus','baik','hebat','tepat', 'puas','bermanfaat','membantu', 'sekali','banget','sangat'}
    stop_words_filtered = stop_words_id - important_words

    words = text.split()
    filtered_words = [word for word in words if word not in stop_words_filtered or word in sentiment_whitelist]

    return ' '.join(filtered_words)

# Terapkan cleaning
print("Membersihkan komentar...")
df['cleaned_comment'] = df['comment'].apply(clean_youtube_comment)
print("Cleaning selesai!")

# Tampilkan contoh hasil
print("\n" + "="*60)
print("CONTOH HASIL CLEANING (3 data pertama):")
print("="*60)
for i in range(min(3, len(df))):
    print(f"\nData {i+1}:")
    print(f"ASLI: {df['comment'].iloc[i][:100]}...")
    print(f"BERSIH: {df['cleaned_comment'].iloc[i][:100]}...")
    print(f"Sentimen: {df['sentimen_relabel'].iloc[i]}")

# Analisis distribusi sentimen
print("\n" + "="*60)
print("DISTRIBUSI SENTIMEN:")
print("="*60)
sentiment_counts = df['sentimen_relabel'].value_counts()
print(sentiment_counts)

# Visualisasi distribusi sentimen (dinonaktifkan untuk training script)
# plt.figure(figsize=(8, 4))
# sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')
# plt.title('Distribusi Sentimen')
# plt.xlabel('Kategori')
# plt.ylabel('Jumlah')
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()

#ambil yg numerik aja
numeric_features = df.select_dtypes(include=['int64']).columns
numeric_features

#cek ada outliers apa ga (visualisasi dinonaktifkan)
# for feature in numeric_features:
#     plt.figure(figsize=(8,4))
#     sns.boxplot(x=df[feature])
#     plt.title(f'Box Plot of {feature}')
#     plt.show()

# hitung IQR
Q1 = df[numeric_features].quantile(0.25)
Q3 = df[numeric_features].quantile(0.75)
IQR = Q3 - Q1

# kondisi baris tanpa outlier
condition = ~(
    (df[numeric_features] < (Q1 - 1.5 * IQR)) |
    (df[numeric_features] > (Q3 + 1.5 * IQR))
).any(axis=1)

# Cek dulu berapa banyak kolom numerik
print("Jumlah kolom numerik:", len(numeric_features))
print("Kolom numerik:", list(numeric_features))

# simpen data asli sebelum standardisasi
df_before = df[numeric_features].copy()

# Standardisasi
scaler = StandardScaler()
df_standardized = scaler.fit_transform(df_before)
df_standardized = pd.DataFrame(df_standardized, columns=numeric_features)

# Tampilkan perbandingan untuk setiap kolom numerik (dinonaktifkan)
# for i, feature in enumerate(numeric_features):
#     plt.figure(figsize=(12, 5))
#     # Sebelum standardisasi
#     plt.subplot(1, 2, 1)
#     sns.histplot(df_before[feature], kde=True, color='blue', alpha=0.6)
#     plt.title(f"Sebelum Standardisasi\n{feature}")
#     plt.xlabel(f"Skor Asli")
#     # Sesudah standardisasi
#     plt.subplot(1, 2, 2)
#     sns.histplot(df_standardized[feature], kde=True, color='red', alpha=0.6)
#     plt.title(f"Sesudah Standardisasi\n{feature}")
#     plt.xlabel("Skor Z (Standardized)")
#     plt.tight_layout()
#     plt.show()

#cek ada duplikasi apa ngga
duplicates = df.duplicated()
print("Baris duplikat: ")
print(df[duplicates])
#hapus data tsb
df = df.drop_duplicates()
print("DataFrame setelah menghapus duplikat")
print(df)

#tf idf
X_text = df['cleaned_comment']
y = df['sentimen_relabel']

tfidf = TfidfVectorizer(
    max_features=6000,
    ngram_range=(1,2),   # penting buat konteks
    min_df=2,
    max_df=0.9
)

X_tfidf = tfidf.fit_transform(X_text)

# Encode label
le = LabelEncoder()
y_encoded = le.fit_transform(df['sentimen_relabel'])

# Split data DARI TF-IDF
X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf,
    y_encoded,
    test_size=0.2,
    random_state=42,
    stratify=y_encoded
)

print("jumlah data:", X_tfidf.shape[0])
print("jumlah data latih:", X_train.shape[0])
print("jumlah data test:", X_test.shape[0])

#krn masi ada imbalance data pakai smote
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

print("Sebelum SMOTE:", Counter(y_train))
print("Sesudah SMOTE:", Counter(y_train_sm))

#modelling pke naive bayes
nb_model = MultinomialNB()
nb_model.fit(X_train_sm, y_train_sm)

#evaluasi
y_pred_nb = nb_model.predict(X_test)

print(classification_report(
    y_test,
    y_pred_nb,
    target_names=le.classes_
))

# Hitung Kata Terbanyak
positive_lexicon = {
    'bagus','baik','mantap','setuju','bermanfaat','membantu','rapi',
    'berhasil','meningkat','positif','patut','nyata','hebat','tepat',
    'inisiatif','layak','keren','oke','optimal'
}

negative_lexicon = {
    'racun','korupsi','korup','gagal','buruk','jelek','parah','busuk',
    'bobrok','bohong','rusak','bahaya','keracunan','kecewa','sampah',
    'ancam','merugikan','jahat','bobol','celaka'
}

def sentiment_word_count(df, sentiment_label, lexicon, top_n=15):
    texts = df[df['sentimen_relabel'] == sentiment_label]['cleaned_comment']
    words = []

    for text in texts:
        for word in text.split():
            if word in lexicon:
                words.append(word)

    return Counter(words).most_common(top_n)

print("\n===== KATA SENTIMEN POSITIF =====")
pos_words = sentiment_word_count(df, 'positif', positive_lexicon)
for word, count in pos_words:
    print(f"{word}: {count}")

print("\n===== KATA SENTIMEN NEGATIF =====")
neg_words = sentiment_word_count(df, 'negatif', negative_lexicon)
for word, count in neg_words:
    print(f"{word}: {count}")

# ================================
# SIMPAN MODEL, VECTORIZER, DAN LABEL ENCODER
# ================================
print("\n" + "="*60)
print("MENYIMPAN MODEL...")
print("="*60)

pickle.dump(nb_model, open('models/model.pkl', 'wb'))
pickle.dump(tfidf, open('models/vectorizer.pkl', 'wb'))
pickle.dump(le, open('models/label_encoder.pkl', 'wb'))

print("âœ… Model berhasil disimpan ke folder 'models/'")
print("   - models/model.pkl")
print("   - models/vectorizer.pkl")
print("   - models/label_encoder.pkl")

# # ================================
# # PREDIKSI KOMENTAR BARU (MANUAL)
# # ================================

# def predict_comment_sentiment(comment_text):
#     # 1. Cleaning
#     cleaned = clean_youtube_comment(comment_text)

#     # 2. TF-IDF transform (PAKAI FIT YANG SAMA!)
#     vectorized = tfidf.transform([cleaned])

#     # 3. Prediksi
#     pred_encoded = nb_model.predict(vectorized)[0]
#     pred_label = le.inverse_transform([pred_encoded])[0]

#     # Fix: Use pred_label instead of undefined 'label'
#     df.loc[len(df)] = {
#         'comment': comment_text,
#         'cleaned_comment': cleaned,
#         'sentimen_relabel': pred_label
#     }

#     return pred_label, cleaned


# # ==== INPUT USER ====
# while True:
#     user_input = input("\nMasukkan komentar (ketik 'exit' untuk keluar): ")

#     if user_input.lower() == 'exit':
#         print("Program selesai.")
#         break

#     label, cleaned_text = predict_comment_sentiment(user_input)

#     print("\nHasil Prediksi:")
#     print("Komentar Bersih :", cleaned_text)
#     print("Sentimen       :", label)

#     if label == 'negatif':
#       neg_words = sentiment_word_count(df, 'negatif', negative_lexicon)
#       print("\nUpdate kata negatif:")
#       print(neg_words)

#     elif label == 'positif':
#       pos_words = sentiment_word_count(df, 'positif', positive_lexicon)
#       print("\nUpdate kata positif:")
#       print(pos_words)